
<!--bash
source ./scripts/assert.sh
-->



<center>
<svg id="gloo-product-logo" viewBox="260 150 1775 480" fill="none" xmlns="http://www.w3.org/2000/svg" alt="Gloo Mesh" style="max-height:100px">
    <style>#gloo-product-logo path{fill:#151927}@media(prefers-color-scheme:dark){#gloo-product-logo path{fill:white}}</style>
    <path fill-rule="evenodd" clip-rule="evenodd" d="M300.935 447.396C328.108 474.569 364.95 489.855 403.379 489.899C441.82 489.84 478.667 474.531 505.832 447.332C532.997 420.133 548.26 383.267 548.271 344.826V315.504H403.309V374.148H484.489C478.443 390.83 467.41 405.248 452.886 415.442C438.363 425.637 421.054 431.114 403.309 431.13C380.428 431.104 358.492 422.003 342.312 405.824C326.133 389.644 317.031 367.707 317.006 344.826C316.992 333.456 319.243 322.197 323.628 311.707C328.013 301.216 334.443 291.704 342.544 283.725C350.575 275.677 360.13 269.309 370.649 264.994C381.169 260.679 392.442 258.503 403.812 258.592C425.041 258.708 445.48 266.664 461.199 280.932L500.546 237.452C474.139 213.531 439.833 200.197 404.203 200.004C385.096 199.857 366.152 203.519 348.476 210.776C330.8 218.033 314.747 228.739 301.256 242.27C287.659 255.686 276.87 271.675 269.518 289.304C262.166 306.934 258.397 325.851 258.432 344.952C258.476 383.381 273.761 420.223 300.935 447.396ZM714.484 446.974C741.552 474.043 778.253 489.27 816.533 489.314C854.829 489.292 891.55 474.075 918.637 447.004C945.724 419.933 960.962 383.22 961.006 344.925C960.962 306.63 945.724 269.917 918.637 242.846C891.55 215.775 854.829 200.558 816.533 200.536C778.253 200.58 741.552 215.807 714.484 242.875C687.415 269.944 672.189 306.644 672.144 344.925C672.189 383.206 687.415 419.906 714.484 446.974ZM755.926 284.313C772.001 268.235 793.798 259.191 816.533 259.165V259.179C839.28 259.187 861.094 268.22 877.186 284.297C893.278 300.373 902.333 322.178 902.362 344.925C902.336 367.672 893.283 389.48 877.19 405.557C861.097 421.634 839.281 430.667 816.533 430.67C793.8 430.644 772.005 421.602 755.931 405.527C739.856 389.453 730.814 367.658 730.788 344.925C730.81 322.189 739.851 300.391 755.926 284.313ZM1024.78 446.979C1051.85 474.048 1088.56 489.273 1126.84 489.314C1165.12 489.27 1201.81 474.045 1228.88 446.979C1255.95 419.914 1271.18 383.217 1271.23 344.939C1271.19 306.656 1255.96 269.952 1228.89 242.88C1201.82 215.809 1165.12 200.58 1126.84 200.536C1088.56 200.576 1051.85 215.801 1024.78 242.87C997.708 269.939 982.48 306.642 982.436 344.925C982.48 383.208 997.708 419.91 1024.78 446.979ZM1066.23 284.312C1082.3 268.235 1104.1 259.191 1126.84 259.165V259.179C1149.57 259.205 1171.37 268.247 1187.44 284.322C1203.52 300.397 1212.56 322.192 1212.58 344.925C1212.56 367.658 1203.52 389.453 1187.44 405.527C1171.37 421.602 1149.57 430.644 1126.84 430.67C1104.1 430.648 1082.3 421.607 1066.23 405.532C1050.15 389.457 1041.11 367.66 1041.08 344.925C1041.11 322.188 1050.15 300.39 1066.23 284.312ZM688.742 472.692C671.919 455.941 658.571 436.031 649.465 414.106C640.36 392.181 635.676 368.673 635.683 344.933V206.283H577.584V344.933C577.564 376.299 583.741 407.36 595.76 436.332C607.779 465.304 625.404 491.616 647.621 513.757L688.742 472.692Z"/>
    <path d="M2008.38 412.919V275.844H2027.57V412.919H2008.38ZM1915.36 412.919V275.844H1934.55V412.919H1915.36ZM1931.42 351.235V333.612H2014.45V351.235H1931.42Z"/>
    <path d="M1820.14 415.269C1812.82 415.269 1806.17 414.42 1800.16 412.723C1794.29 411.026 1789.13 408.741 1784.69 405.869C1780.38 402.867 1776.79 399.538 1773.92 395.882C1771.05 392.227 1769.03 388.376 1767.85 384.329L1786.45 378.259C1788.41 383.611 1792.13 388.245 1797.62 392.162C1803.1 396.078 1809.82 398.102 1817.79 398.232C1827.45 398.232 1835.02 396.339 1840.5 392.553C1846.11 388.637 1848.92 383.48 1848.92 377.084C1848.92 371.209 1846.51 366.444 1841.68 362.789C1836.85 359.003 1830.45 356.131 1822.49 354.173L1806.62 350.06C1800.36 348.363 1794.61 345.948 1789.39 342.815C1784.3 339.682 1780.25 335.7 1777.25 330.87C1774.25 326.04 1772.75 320.23 1772.75 313.442C1772.75 300.909 1776.79 291.118 1784.89 284.069C1793.11 277.019 1804.93 273.495 1820.33 273.495C1829.21 273.495 1836.91 274.931 1843.44 277.803C1849.97 280.544 1855.32 284.199 1859.5 288.769C1863.8 293.338 1866.87 298.364 1868.7 303.847L1850.1 310.113C1847.88 304.238 1844.09 299.539 1838.74 296.014C1833.39 292.359 1826.79 290.531 1818.96 290.531C1810.74 290.531 1804.21 292.489 1799.38 296.406C1794.68 300.322 1792.33 305.674 1792.33 312.463C1792.33 318.207 1794.16 322.646 1797.81 325.779C1801.6 328.781 1806.62 331.066 1812.89 332.632L1828.75 336.549C1841.55 339.682 1851.4 344.904 1858.32 352.214C1865.37 359.395 1868.89 367.293 1868.89 375.909C1868.89 383.35 1867 390.073 1863.22 396.078C1859.56 401.953 1854.08 406.653 1846.77 410.177C1839.59 413.572 1830.71 415.269 1820.14 415.269Z"/>
    <path d="M1640.6 412.919V275.844H1723.04V293.468H1659.79V334.199H1716.78V351.431H1659.79V395.295H1723.44V412.919H1640.6Z"/>
    <path d="M1454.84 412.919V275.844H1475.21L1520.64 343.011L1565.87 275.844H1586.24V412.919H1567.05V306.784L1520.44 375.517L1474.03 306.98V412.919H1454.84Z"/>
    <path fill-rule="evenodd" clip-rule="evenodd" d="M1354.32 488.104L1354.32 201.133L1356.82 201.133L1356.82 488.104L1354.32 488.104Z"/>
</svg>
</center>

# <center>Gloo Mesh Core (2.6.6) Ambient</center>



## Table of Contents
* [Introduction](#introduction)
* [Lab 1 - Deploy KinD clusters](#lab-1---deploy-kind-clusters-)
* [Lab 2 - Deploy and register Gloo Mesh](#lab-2---deploy-and-register-gloo-mesh-)
* [Lab 3 - Configure common trust certificates in both clusters](#lab-3---configure-common-trust-certificates-in-both-clusters-)
* [Lab 4 - Deploy Istio using Helm](#lab-4---deploy-istio-using-helm-)
* [Lab 5 - Deploy the httpbin demo app](#lab-5---deploy-the-httpbin-demo-app-)
* [Lab 6 - Deploy the httpbin demo app](#lab-6---deploy-the-httpbin-demo-app-)
* [Lab 7 - Deploy the clients to make requests to other services](#lab-7---deploy-the-clients-to-make-requests-to-other-services-)
* [Lab 8 - Link Clusters](#lab-8---link-clusters-)



## Introduction <a name="introduction"></a>

[Gloo Mesh Core](https://www.solo.io/products/gloo-mesh/) is a management plane that makes it easy to operate [Istio](https://istio.io) and adds additional features to Ambient.

Gloo Mesh Core works with community [Istio](https://istio.io/) out of the box.
You get instant insights into your Istio environment through a custom dashboard.
Observability pipelines let you analyze many data sources that you already have.
You can even automate installing and upgrading Istio with the Gloo lifecycle manager, on one or many Kubernetes clusters deployed anywhere.

But Gloo Mesh Core includes more than tooling to complement an existing Istio installation.
You can also replace community Istio with Solo's hardened Istio images. These images unlock enterprise-level support.
Later, you might choose to upgrade seamlessly to Gloo Mesh Enterprise for a full-stack service mesh and API gateway solution.
This approach lets you scale as you need more advanced routing and security features.

### Istio and Ambient support

The Gloo Mesh Core subscription includes end-to-end Istio support:

* Upstream feature development
* CI/CD-ready automated installation and upgrade
* End-to-end Istio support and CVE security patching
* Long-term n-4 version support with Solo images
* Special image builds for distroless and FIPS compliance
* 24x7 production support and one-hour Severity 1 SLA
* Ambient support for Istio
* L7 Telemetry support in Ztunnel

### Gloo Mesh Core overview

Gloo Mesh Core provides many unique features, including:

* Single pane of glass for operational management of Istio, including global observability
* Insights based on environment checks with corrective actions and best practices
* [Cilium](https://cilium.io/) support
* Seamless migration to full-stack service mesh

### Want to learn more about Gloo Mesh Core?

You can find more information about Gloo Mesh Core in the official documentation: <https://docs.solo.io/gloo-mesh-core>




## Lab 1 - Deploy KinD clusters <a name="lab-1---deploy-kind-clusters-"></a>


Clone this repository and go to the directory where this `README.md` file is.

Set the context environment variables:

```bash
export MGMT=cluster1
export CLUSTER1=cluster1
export CLUSTER2=cluster2
```

Run the following commands to deploy two Kubernetes clusters using [Kind](https://kind.sigs.k8s.io/):

```bash
./scripts/deploy-aws-with-calico.sh 1 cluster1 us-west us-west-1
./scripts/deploy-aws-with-calico.sh 2 cluster2 us-west us-west-2
```

Then run the following commands to wait for all the Pods to be ready:

```bash
./scripts/check.sh cluster1
./scripts/check.sh cluster2
```

**Note:** If you run the `check.sh` script immediately after the `deploy.sh` script, you may see a jsonpath error. If that happens, simply wait a few seconds and try again.

Once the `check.sh` script completes, when you execute the `kubectl get pods -A` command, you should see the following:

```
NAMESPACE            NAME                                          READY   STATUS    RESTARTS   AGE
kube-system          calico-kube-controllers-59d85c5c84-sbk4k      1/1     Running   0          4h26m
kube-system          calico-node-przxs                             1/1     Running   0          4h26m
kube-system          coredns-6955765f44-ln8f5                      1/1     Running   0          4h26m
kube-system          coredns-6955765f44-s7xxx                      1/1     Running   0          4h26m
kube-system          etcd-cluster1-control-plane                   1/1     Running   0          4h27m
kube-system          kube-apiserver-cluster1-control-plane         1/1     Running   0          4h27m
kube-system          kube-controller-manager-cluster1-control-plane1/1     Running   0          4h27m
kube-system          kube-proxy-ksvzw                              1/1     Running   0          4h26m
kube-system          kube-scheduler-cluster1-control-plane         1/1     Running   0          4h27m
local-path-storage   local-path-provisioner-58f6947c7-lfmdx        1/1     Running   0          4h26m
metallb-system       controller-5c9894b5cd-cn9x2                   1/1     Running   0          4h26m
metallb-system       speaker-d7jkp                                 1/1     Running   0          4h26m
```

**Note:** The CNI pods might be different, depending on which CNI you have deployed.

You can see that your currently connected to this cluster by executing the `kubectl config get-contexts` command:

```
CURRENT   NAME         CLUSTER         AUTHINFO   NAMESPACE
          cluster1     kind-cluster1   cluster1
*         cluster2     kind-cluster2   cluster2
```

Run the following command to make `cluster1` the current cluster.

```bash
kubectl config use-context ${MGMT}
```
<!--bash
cat <<'EOF' > ./test.js
const helpers = require('./tests/chai-exec');

describe("Clusters are healthy", () => {
    const clusters = [process.env.MGMT, process.env.CLUSTER1, process.env.CLUSTER2];
    clusters.forEach(cluster => {
        it(`Cluster ${cluster} is healthy`, () => helpers.k8sObjectIsPresent({ context: cluster, namespace: "default", k8sType: "service", k8sObj: "kubernetes" }));
    });
});
EOF
echo "executing test dist/gloo-mesh-2-0-workshop/build/templates/steps/deploy-kind-clusters/tests/cluster-healthy.test.js.liquid"
timeout --signal=INT 3m mocha ./test.js --timeout 10000 --retries=120 --bail || { DEBUG_MODE=true mocha ./test.js --timeout 120000; exit 1; }
-->



## Lab 2 - Deploy and register Gloo Mesh <a name="lab-2---deploy-and-register-gloo-mesh-"></a>
[<img src="https://img.youtube.com/vi/djfFiepK4GY/maxresdefault.jpg" alt="VIDEO LINK" width="560" height="315"/>](https://youtu.be/djfFiepK4GY "Video Link")


Before we get started, let's install the `meshctl` CLI:

```bash
export GLOO_MESH_VERSION=v2.6.6
curl -sL https://run.solo.io/meshctl/install | sh -
export PATH=$HOME/.gloo-mesh/bin:$PATH
```
<!--bash
cat <<'EOF' > ./test.js
var chai = require('chai');
var expect = chai.expect;

describe("Required environment variables should contain value", () => {
  afterEach(function(done){
    if(this.currentTest.currentRetry() > 0){
      process.stdout.write(".");
       setTimeout(done, 1000);
    } else {
      done();
    }
  });

  it("Context environment variables should not be empty", () => {
    expect(process.env.MGMT).not.to.be.empty
    expect(process.env.CLUSTER1).not.to.be.empty
    expect(process.env.CLUSTER2).not.to.be.empty
  });

  it("Gloo Mesh licence environment variables should not be empty", () => {
    expect(process.env.GLOO_MESH_LICENSE_KEY).not.to.be.empty
  });
});
EOF
echo "executing test dist/gloo-mesh-2-0-workshop/build/templates/steps/deploy-and-register-gloo-mesh/tests/environment-variables.test.js.liquid"
timeout --signal=INT 3m mocha ./test.js --timeout 10000 --retries=120 --bail || { DEBUG_MODE=true mocha ./test.js --timeout 120000; exit 1; }
-->
Run the following commands to deploy the Gloo Mesh management plane:

```bash
kubectl --context ${MGMT} create ns gloo-mesh

helm upgrade --install gloo-platform-crds gloo-platform-crds \
  --repo https://storage.googleapis.com/gloo-platform/helm-charts \
  --namespace gloo-mesh \
  --kube-context ${MGMT} \
  --set featureGates.insightsConfiguration=true \
  --version 2.6.6

helm upgrade --install gloo-platform-mgmt gloo-platform \
  --repo https://storage.googleapis.com/gloo-platform/helm-charts \
  --namespace gloo-mesh \
  --kube-context ${MGMT} \
  --version 2.6.6 \
  -f -<<EOF
licensing:
  glooTrialLicenseKey: ${GLOO_MESH_LICENSE_KEY}
common:
  cluster: cluster1
experimental:
  ambientEnabled: true
glooInsightsEngine:
  enabled: true
glooMgmtServer:
  enabled: true
  ports:
    healthcheck: 8091
  registerCluster: true
prometheus:
  enabled: true
redis:
  deployment:
    enabled: true
telemetryGateway:
  enabled: true
  service:
    type: LoadBalancer
glooUi:
  enabled: true
  serviceType: LoadBalancer
telemetryCollector:
  enabled: true
  config:
    exporters:
      otlp:
        endpoint: gloo-telemetry-gateway:4317
glooAgent:
  enabled: true
  relay:
    serverAddress: gloo-mesh-mgmt-server:9900
    authority: gloo-mesh-mgmt-server.gloo-mesh
featureGates:
  istioLifecycleAgent: true
EOF

kubectl --context ${MGMT} -n gloo-mesh rollout status deploy/gloo-mesh-mgmt-server
```

<!--bash
kubectl wait --context ${MGMT} --for=condition=Ready -n gloo-mesh --all pod
timeout 2m bash -c "until [[ \$(kubectl --context ${MGMT} -n gloo-mesh get svc gloo-mesh-mgmt-server -o json | jq '.status.loadBalancer | length') -gt 0 ]]; do
  sleep 1
done"
-->

Then, you need to set the environment variable to tell the Gloo Mesh agents how to communicate with the management plane:
<!--bash
cat <<'EOF' > ./test.js

const helpers = require('./tests/chai-exec');

describe("MGMT server is healthy", () => {
  let cluster = process.env.MGMT;
  let deployments = ["gloo-mesh-mgmt-server","gloo-mesh-redis","gloo-telemetry-gateway","prometheus-server"];
  deployments.forEach(deploy => {
    it(deploy + ' pods are ready in ' + cluster, () => helpers.checkDeployment({ context: cluster, namespace: "gloo-mesh", k8sObj: deploy }));
  });
});
EOF
echo "executing test dist/gloo-mesh-2-0-workshop/build/templates/steps/deploy-and-register-gloo-mesh/tests/check-deployment.test.js.liquid"
timeout --signal=INT 3m mocha ./test.js --timeout 10000 --retries=120 --bail || { DEBUG_MODE=true mocha ./test.js --timeout 120000; exit 1; }
-->
<!--bash
cat <<'EOF' > ./test.js
const chaiExec = require("@jsdevtools/chai-exec");
var chai = require('chai');
var expect = chai.expect;
chai.use(chaiExec);

afterEach(function (done) {
  if (this.currentTest.currentRetry() > 0) {
    process.stdout.write(".");
    setTimeout(done, 1000);
  } else {
    done();
  }
});
EOF
echo "executing test dist/gloo-mesh-2-0-workshop/build/templates/steps/deploy-and-register-gloo-mesh/tests/get-gloo-mesh-mgmt-server-ip.test.js.liquid"
timeout --signal=INT 3m mocha ./test.js --timeout 10000 --retries=120 --bail || { DEBUG_MODE=true mocha ./test.js --timeout 120000; exit 1; }
-->

```bash
export ENDPOINT_GLOO_MESH=$(kubectl --context ${MGMT} -n gloo-mesh get svc gloo-mesh-mgmt-server -o jsonpath='{.status.loadBalancer.ingress[0].*}'):9900
export HOST_GLOO_MESH=$(echo ${ENDPOINT_GLOO_MESH%:*})
export ENDPOINT_TELEMETRY_GATEWAY=$(kubectl --context ${MGMT} -n gloo-mesh get svc gloo-telemetry-gateway -o jsonpath='{.status.loadBalancer.ingress[0].*}'):4317
export ENDPOINT_GLOO_MESH_UI=$(kubectl --context ${MGMT} -n gloo-mesh get svc gloo-mesh-ui -o jsonpath='{.status.loadBalancer.ingress[0].*}'):8090
```

Check that the variables have correct values:
```
echo $HOST_GLOO_MESH
echo $ENDPOINT_GLOO_MESH
```

<!--bash
cat <<'EOF' > ./test.js
const dns = require('dns');
const chaiHttp = require("chai-http");
const chai = require("chai");
const expect = chai.expect;
chai.use(chaiHttp);
const { waitOnFailedTest } = require('./tests/utils');

afterEach(function(done) { waitOnFailedTest(done, this.currentTest.currentRetry())});

describe("Address '" + process.env.HOST_GLOO_MESH + "' can be resolved in DNS", () => {
    it(process.env.HOST_GLOO_MESH + ' can be resolved', (done) => {
        return dns.lookup(process.env.HOST_GLOO_MESH, (err, address, family) => {
            expect(address).to.be.an.ip;
            done();
        });
    });
});
EOF
echo "executing test ./gloo-mesh-2-0/tests/can-resolve.test.js.liquid"
timeout --signal=INT 3m mocha ./test.js --timeout 10000 --retries=120 --bail || { DEBUG_MODE=true mocha ./test.js --timeout 120000; exit 1; }
-->
Finally, you need to register the cluster(s).


And here is how you register the second one:

```bash
kubectl apply --context ${MGMT} -f - <<EOF
apiVersion: admin.gloo.solo.io/v2
kind: KubernetesCluster
metadata:
  name: cluster2
  namespace: gloo-mesh
spec:
  clusterDomain: cluster.local
EOF

kubectl --context ${CLUSTER2} create ns gloo-mesh

kubectl get secret relay-root-tls-secret -n gloo-mesh --context ${MGMT} -o jsonpath='{.data.ca\.crt}' | base64 -d > ca.crt
kubectl create secret generic relay-root-tls-secret -n gloo-mesh --context ${CLUSTER2} --from-file ca.crt=ca.crt
rm ca.crt

kubectl get secret relay-identity-token-secret -n gloo-mesh --context ${MGMT} -o jsonpath='{.data.token}' | base64 -d > token
kubectl create secret generic relay-identity-token-secret -n gloo-mesh --context ${CLUSTER2} --from-file token=token
rm token

helm upgrade --install gloo-platform-crds gloo-platform-crds \
  --repo https://storage.googleapis.com/gloo-platform/helm-charts \
  --namespace gloo-mesh \
  --kube-context ${CLUSTER2} \
  --version 2.6.6

helm upgrade --install gloo-platform-agent gloo-platform \
  --repo https://storage.googleapis.com/gloo-platform/helm-charts \
  --namespace gloo-mesh \
  --kube-context ${CLUSTER2} \
  --version 2.6.6 \
  -f -<<EOF
common:
  cluster: cluster2
glooAgent:
  enabled: true
  relay:
    serverAddress: "${ENDPOINT_GLOO_MESH}"
    authority: gloo-mesh-mgmt-server.gloo-mesh
telemetryCollector:
  enabled: true
  config:
    exporters:
      otlp:
        endpoint: "${ENDPOINT_TELEMETRY_GATEWAY}"
glooAnalyzer:
  enabled: true
EOF
```

You can check the cluster(s) have been registered correctly using the following commands:
```
meshctl --kubecontext ${MGMT} check
```

```
pod=$(kubectl --context ${MGMT} -n gloo-mesh get pods -l app=gloo-mesh-mgmt-server -o jsonpath='{.items[0].metadata.name}')
kubectl --context ${MGMT} -n gloo-mesh debug -q -i ${pod} --image=curlimages/curl -- curl -s http://localhost:9091/metrics | grep relay_push_clients_connected
```

You should get an output similar to this:
```,nocopy
# HELP relay_push_clients_connected Current number of connected Relay push clients (Relay Agents).
# TYPE relay_push_clients_connected gauge
relay_push_clients_connected{cluster="cluster1"} 1
relay_push_clients_connected{cluster="cluster2"} 1
```
Finally, you need to specify which gateways you want to use for cross cluster traffic:

```bash
kubectl apply --context ${MGMT} -f - <<EOF
apiVersion: admin.gloo.solo.io/v2
kind: WorkspaceSettings
metadata:
  name: global
  namespace: gloo-mesh
spec:
  options:
    eastWestGateways:
      - selector:
          labels:
            istio: eastwestgateway
EOF
```
<!--bash
cat <<'EOF' > ./test.js
var chai = require('chai');
var expect = chai.expect;
const helpers = require('./tests/chai-exec');
describe("Cluster registration", () => {
  it("cluster1 is registered", () => {
    podName = helpers.getOutputForCommand({ command: "kubectl -n gloo-mesh get pods -l app=gloo-mesh-mgmt-server -o jsonpath='{.items[0].metadata.name}' --context " + process.env.MGMT }).replaceAll("'", "");
    command = helpers.getOutputForCommand({ command: "kubectl --context " + process.env.MGMT + " -n gloo-mesh debug -q -i " + podName + " --image=curlimages/curl -- curl -s http://localhost:9091/metrics" }).replaceAll("'", "");
    expect(command).to.contain("cluster1");
  });
  it("cluster2 is registered", () => {
    podName = helpers.getOutputForCommand({ command: "kubectl -n gloo-mesh get pods -l app=gloo-mesh-mgmt-server -o jsonpath='{.items[0].metadata.name}' --context " + process.env.MGMT }).replaceAll("'", "");
    command = helpers.getOutputForCommand({ command: "kubectl --context " + process.env.MGMT + " -n gloo-mesh debug -q -i " + podName + " --image=curlimages/curl -- curl -s http://localhost:9091/metrics" }).replaceAll("'", "");
    expect(command).to.contain("cluster2");
  });
});
EOF
echo "executing test dist/gloo-mesh-2-0-workshop/build/templates/steps/deploy-and-register-gloo-mesh/tests/cluster-registration.test.js.liquid"
timeout --signal=INT 3m mocha ./test.js --timeout 10000 --retries=120 --bail || { DEBUG_MODE=true mocha ./test.js --timeout 120000; exit 1; }
-->



## Lab 3 - Configure common trust certificates in both clusters <a name="lab-3---configure-common-trust-certificates-in-both-clusters-"></a>

Create intermediate CAs in both clusters and the Root CA.

```bash
echo "Generating new certificates"
mkdir -p "./certs/${CLUSTER1}"
mkdir -p "./certs/${CLUSTER2}"

if ! [ -x "$(command -v step)" ]; then
  echo 'Error: Install the smallstep cli (https://github.com/smallstep/cli)'
  exit 1
fi

step certificate create root.istio.ca ./certs/root-cert.pem ./certs/root-ca.key \
  --profile root-ca --no-password --insecure --san root.istio.ca \
  --not-after 87600h --kty RSA

step certificate create $CLUSTER1 \
  ./certs/$CLUSTER1/ca-cert.pem \
  ./certs/$CLUSTER1/ca-key.pem \
  --ca ./certs/root-cert.pem \
  --ca-key ./certs/root-ca.key \
  --profile intermediate-ca \
  --not-after 87600h \
  --no-password \
  --san $CLUSTER1 \
  --kty RSA \
  --insecure

step certificate create $CLUSTER2 \
  ./certs/$CLUSTER2/ca-cert.pem \
  ./certs/$CLUSTER2/ca-key.pem \
  --ca ./certs/root-cert.pem \
  --ca-key ./certs/root-ca.key \
  --profile intermediate-ca \
  --not-after 87600h \
  --no-password \
  --san $CLUSTER2 \
  --kty RSA \
  --insecure

cat ./certs/$CLUSTER1/ca-cert.pem ./certs/root-cert.pem > ./certs/$CLUSTER1/cert-chain.pem
cat ./certs/$CLUSTER2/ca-cert.pem ./certs/root-cert.pem > ./certs/$CLUSTER2/cert-chain.pem
```


Now let's apply the certificates to the clusters.
```bash
kubectl --context="${CLUSTER1}" create namespace istio-system || true
kubectl --context="${CLUSTER1}" create secret generic cacerts -n istio-system \
  --from-file=./certs/$CLUSTER1/ca-cert.pem \
  --from-file=./certs/$CLUSTER1/ca-key.pem \
  --from-file=./certs/root-cert.pem \
  --from-file=./certs/$CLUSTER1/cert-chain.pem

kubectl --context="${CLUSTER2}" create namespace istio-system || true
kubectl --context="${CLUSTER2}" create secret generic cacerts -n istio-system \
  --from-file=./certs/$CLUSTER2/ca-cert.pem \
  --from-file=./certs/$CLUSTER2/ca-key.pem \
  --from-file=./certs/root-cert.pem \
  --from-file=./certs/$CLUSTER2/cert-chain.pem
```



## Lab 4 - Deploy Istio using Helm <a name="lab-4---deploy-istio-using-helm-"></a>


It is convenient to have the `istioctl` command line tool installed on your local machine. If you don't have it installed, you can install it by following the instructions below.

<details>
  <summary>Install <code>istioctl</code></summary>

Install `istioctl` if not already installed as it will be useful in some of the labs that follow.

```bash
curl -L https://istio.io/downloadIstio | sh -

if [ -d "istio-"*/ ]; then
  cd istio-*/
  export PATH=$PWD/bin:$PATH
  cd ..
fi
```

That's it!
</details>

<!--bash
cat <<'EOF' > ./test.js
const chaiExec = require("@jsdevtools/chai-exec");
var chai = require('chai');
var expect = chai.expect;
chai.use(chaiExec);

afterEach(function (done) {
  if (this.currentTest.currentRetry() > 0) {
    process.stdout.write(".");
    setTimeout(done, 1000);
  } else {
    done();
  }
});
describe("istio_version is at least 1.23.0", () => {
  it("version should be at least 1.23.0", () => {
    // Compare the string istio_version to the number 1.23.0
    // example 1.23.0-patch0 is valid, but 1.22.6 is not
    let version = "1.24-alpha.a2295ca05a358e7c8e9edbbd3f500c8b4eb11864";
    let versionParts = version.split('-')[0].split('.');
    let major = parseInt(versionParts[0]);
    let minor = parseInt(versionParts[1]);
    let patch = parseInt(versionParts[2]);
    let minMajor = 1;
    let minMinor = 23;
    let minPatch = 0;
    expect(major).to.be.at.least(minMajor);
    if (major === minMajor) {
      expect(minor).to.be.at.least(minMinor);
      if (minor === minMinor) {
        expect(patch).to.be.at.least(minPatch);
      }
    }
  });
});
EOF
echo "executing test dist/gloo-mesh-2-0-workshop/build/templates/steps/deploy-istio-helm/tests/istio-version.test.js.liquid"
timeout --signal=INT 3m mocha ./test.js --timeout 10000 --retries=120 --bail || { DEBUG_MODE=true mocha ./test.js --timeout 120000; exit 1; }
-->

Let's create Kubernetes services for the gateways:

```bash
kubectl --context ${CLUSTER1} create ns istio-gateways

kubectl apply --context ${CLUSTER1} -f - <<EOF
apiVersion: v1
kind: Service
metadata:
  labels:
    app: istio-ingressgateway
    istio: ingressgateway
  name: istio-ingressgateway
  namespace: istio-gateways
spec:
  ports:
  - name: http2
    port: 80
    protocol: TCP
    targetPort: 80
  - name: https
    port: 443
    protocol: TCP
    targetPort: 443
  selector:
    app: istio-ingressgateway
    istio: ingressgateway
  type: LoadBalancer
EOF

kubectl apply --context ${CLUSTER1} -f - <<EOF
apiVersion: v1
kind: Service
metadata:
  labels:
    app: istio-ingressgateway
    istio: eastwestgateway
  name: istio-eastwestgateway
  namespace: istio-gateways
spec:
  ports:
  - name: status-port
    port: 15021
    protocol: TCP
    targetPort: 15021
  - name: tls
    port: 15443
    protocol: TCP
    targetPort: 15443
  - name: hbone
    port: 15008
    protocol: TCP
    targetPort: 15008
  - name: https
    port: 16443
    protocol: TCP
    targetPort: 16443
  - name: tls-spire
    port: 8081
    protocol: TCP
    targetPort: 8081
  - name: tls-otel
    port: 4317
    protocol: TCP
    targetPort: 4317
  - name: grpc-cacert
    port: 31338
    protocol: TCP
    targetPort: 31338
  - name: grpc-ew-bootstrap
    port: 31339
    protocol: TCP
    targetPort: 31339
  - name: tcp-istiod
    port: 15012
    protocol: TCP
    targetPort: 15012
  - name: tcp-webhook
    port: 15017
    protocol: TCP
    targetPort: 15017
  selector:
    app: istio-ingressgateway
    istio: eastwestgateway
  type: LoadBalancer
EOF
kubectl --context ${CLUSTER2} create ns istio-gateways

kubectl apply --context ${CLUSTER2} -f - <<EOF
apiVersion: v1
kind: Service
metadata:
  labels:
    app: istio-ingressgateway
    istio: ingressgateway
  name: istio-ingressgateway
  namespace: istio-gateways
spec:
  ports:
  - name: http2
    port: 80
    protocol: TCP
    targetPort: 80
  - name: https
    port: 443
    protocol: TCP
    targetPort: 443
  selector:
    app: istio-ingressgateway
    istio: ingressgateway
  type: LoadBalancer
EOF

kubectl apply --context ${CLUSTER2} -f - <<EOF
apiVersion: v1
kind: Service
metadata:
  labels:
    app: istio-ingressgateway
    istio: eastwestgateway
  name: istio-eastwestgateway
  namespace: istio-gateways
spec:
  ports:
  - name: status-port
    port: 15021
    protocol: TCP
    targetPort: 15021
  - name: tls
    port: 15443
    protocol: TCP
    targetPort: 15443
  - name: hbone
    port: 15008
    protocol: TCP
    targetPort: 15008
  - name: https
    port: 16443
    protocol: TCP
    targetPort: 16443
  - name: tls-spire
    port: 8081
    protocol: TCP
    targetPort: 8081
  - name: tls-otel
    port: 4317
    protocol: TCP
    targetPort: 4317
  - name: grpc-cacert
    port: 31338
    protocol: TCP
    targetPort: 31338
  - name: grpc-ew-bootstrap
    port: 31339
    protocol: TCP
    targetPort: 31339
  - name: tcp-istiod
    port: 15012
    protocol: TCP
    targetPort: 15012
  - name: tcp-webhook
    port: 15017
    protocol: TCP
    targetPort: 15017
  selector:
    app: istio-ingressgateway
    istio: eastwestgateway
  type: LoadBalancer
EOF
```

Let's deploy Istio using Helm in cluster1. We'll install the base Istio components, the Istiod control plane, the Istio CNI, the ztunnel, and the ingress/eastwest gateways.
For private registries, let's first load the images into kind:
```bash
KIND_NAME=$(kubectl config get-contexts ${CLUSTER1} | grep ${CLUSTER1} | awk '{printf $3}' | cut -d'-' -f2)

for image in pilot install-cni ztunnel proxyv2; do
  docker pull "us-docker.pkg.dev/istio-enterprise-private/internal-istio-builds/${image}:1.24-alpha.a2295ca05a358e7c8e9edbbd3f500c8b4eb11864"
  docker pull "us-docker.pkg.dev/istio-enterprise-private/internal-istio-builds/${image}:1.24-alpha.a2295ca05a358e7c8e9edbbd3f500c8b4eb11864-distroless"
  kind load docker-image --name "$KIND_NAME" "us-docker.pkg.dev/istio-enterprise-private/internal-istio-builds/${image}:1.24-alpha.a2295ca05a358e7c8e9edbbd3f500c8b4eb11864"
  kind load docker-image --name "$KIND_NAME" "us-docker.pkg.dev/istio-enterprise-private/internal-istio-builds/${image}:1.24-alpha.a2295ca05a358e7c8e9edbbd3f500c8b4eb11864-distroless"
done
```



```bash
helm upgrade --install istio-base oci://us-docker.pkg.dev/istio-enterprise-private/internal-istio-helm/base \
--namespace istio-system \
--kube-context=${CLUSTER1} \
--version 1.24-alpha.a2295ca05a358e7c8e9edbbd3f500c8b4eb11864 \
--create-namespace \
-f - <<EOF
defaultRevision: ""
profile: ambient
EOF

helm upgrade --install istiod- oci://us-docker.pkg.dev/istio-enterprise-private/internal-istio-helm/istiod \
--namespace istio-system \
--kube-context=${CLUSTER1} \
--version 1.24-alpha.a2295ca05a358e7c8e9edbbd3f500c8b4eb11864 \
--create-namespace \
-f - <<EOF
global:
  hub: us-docker.pkg.dev/istio-enterprise-private/internal-istio-builds
  proxy:
    clusterDomain: cluster.local
  tag: 1.24-alpha.a2295ca05a358e7c8e9edbbd3f500c8b4eb11864
  multiCluster:
    clusterName: cluster1
profile: ambient
istio_cni:
  enabled: true
meshConfig:
  accessLogFile: /dev/stdout
  defaultConfig:
    proxyMetadata:
      ISTIO_META_DNS_AUTO_ALLOCATE: "true"
      ISTIO_META_DNS_CAPTURE: "true"
  trustDomain: cluster.local
pilot:
  enabled: true
  env:
    PILOT_ENABLE_IP_AUTOALLOCATE: "true"
    PILOT_ENABLE_K8S_SELECT_WORKLOAD_ENTRIES: "false"
    PILOT_SKIP_VALIDATE_TRUST_DOMAIN: "true"
  podLabels:
    hack: eastwest
  platforms:
    peering:
      enabled: true
EOF

helm upgrade --install istio-cni oci://us-docker.pkg.dev/istio-enterprise-private/internal-istio-helm/cni \
--namespace kube-system \
--kube-context=${CLUSTER1} \
--version 1.24-alpha.a2295ca05a358e7c8e9edbbd3f500c8b4eb11864 \
--create-namespace \
-f - <<EOF
global:
  hub: us-docker.pkg.dev/istio-enterprise-private/internal-istio-builds
  proxy: 1.24-alpha.a2295ca05a358e7c8e9edbbd3f500c8b4eb11864
profile: ambient
cni:
  ambient:
    dnsCapture: true
  excludeNamespaces:
  - istio-system
  - kube-system
EOF

helm upgrade --install ztunnel oci://us-docker.pkg.dev/istio-enterprise-private/internal-istio-helm/ztunnel \
--namespace istio-system \
--kube-context=${CLUSTER1} \
--version 1.24-alpha.a2295ca05a358e7c8e9edbbd3f500c8b4eb11864 \
--create-namespace \
-f - <<EOF
configValidation: true
enabled: true
env:
  L7_ENABLED: "true"
  NETWORK: cluster1
hub: us-docker.pkg.dev/istio-enterprise-private/internal-istio-builds
istioNamespace: istio-system
multiCluster:
  clusterName: cluster1
namespace: istio-system
profile: ambient
proxy:
  clusterDomain: cluster.local
tag: 1.24-alpha.a2295ca05a358e7c8e9edbbd3f500c8b4eb11864
terminationGracePeriodSeconds: 29
variant: distroless
EOF

helm upgrade --install istio-ingressgateway- oci://us-docker.pkg.dev/istio-enterprise-private/internal-istio-helm/gateway \
--namespace istio-gateways \
--kube-context=${CLUSTER1} \
--version 1.24-alpha.a2295ca05a358e7c8e9edbbd3f500c8b4eb11864 \
--create-namespace \
-f - <<EOF
autoscaling:
  enabled: false
profile: ambient
imagePullPolicy: IfNotPresent
labels:
  app: istio-ingressgateway
  istio: ingressgateway
service:
  type: None
EOF

helm upgrade --install istio-eastwestgateway- oci://us-docker.pkg.dev/istio-enterprise-private/internal-istio-helm/gateway \
--namespace istio-gateways \
--kube-context=${CLUSTER1} \
--version 1.24-alpha.a2295ca05a358e7c8e9edbbd3f500c8b4eb11864 \
--create-namespace \
-f - <<EOF
autoscaling:
  enabled: false
profile: ambient
imagePullPolicy: IfNotPresent
env:
  ISTIO_META_REQUESTED_NETWORK_VIEW: cluster1
  ISTIO_META_ROUTER_MODE: sni-dnat
labels:
  app: istio-ingressgateway
  istio: eastwestgateway
  topology.istio.io/network: cluster1
service:
  type: None
EOF
```
The Gateway APIs do not come installed by default on most Kubernetes clusters. Install the Gateway API CRDs if they are not present:
```bash
kubectl --context ${CLUSTER1} get crd gateways.gateway.networking.k8s.io &> /dev/null || \
  { kubectl kustomize "github.com/kubernetes-sigs/gateway-api/config/crd?ref=v1.1.0" | kubectl --context ${CLUSTER1} apply -f -; }
kubectl --context ${CLUSTER2} get crd gateways.gateway.networking.k8s.io &> /dev/null || \
  { kubectl kustomize "github.com/kubernetes-sigs/gateway-api/config/crd?ref=v1.1.0" | kubectl --context ${CLUSTER2} apply -f -; }
```
  
Let's deploy Istio using Helm in cluster2. We'll install the base Istio components, the Istiod control plane, the Istio CNI, the ztunnel, and the ingress/eastwest gateways.
For private registries, let's first load the images into kind:
```bash
KIND_NAME=$(kubectl config get-contexts ${CLUSTER2} | grep ${CLUSTER2} | awk '{printf $3}' | cut -d'-' -f2)

for image in pilot install-cni ztunnel proxyv2; do
  docker pull "us-docker.pkg.dev/istio-enterprise-private/internal-istio-builds/${image}:1.24-alpha.a2295ca05a358e7c8e9edbbd3f500c8b4eb11864"
  docker pull "us-docker.pkg.dev/istio-enterprise-private/internal-istio-builds/${image}:1.24-alpha.a2295ca05a358e7c8e9edbbd3f500c8b4eb11864-distroless"
  kind load docker-image --name "$KIND_NAME" "us-docker.pkg.dev/istio-enterprise-private/internal-istio-builds/${image}:1.24-alpha.a2295ca05a358e7c8e9edbbd3f500c8b4eb11864"
  kind load docker-image --name "$KIND_NAME" "us-docker.pkg.dev/istio-enterprise-private/internal-istio-builds/${image}:1.24-alpha.a2295ca05a358e7c8e9edbbd3f500c8b4eb11864-distroless"
done
```



```bash
helm upgrade --install istio-base oci://us-docker.pkg.dev/istio-enterprise-private/internal-istio-helm/base \
--namespace istio-system \
--kube-context=${CLUSTER2} \
--version 1.24-alpha.a2295ca05a358e7c8e9edbbd3f500c8b4eb11864 \
--create-namespace \
-f - <<EOF
defaultRevision: ""
profile: ambient
EOF

helm upgrade --install istiod- oci://us-docker.pkg.dev/istio-enterprise-private/internal-istio-helm/istiod \
--namespace istio-system \
--kube-context=${CLUSTER2} \
--version 1.24-alpha.a2295ca05a358e7c8e9edbbd3f500c8b4eb11864 \
--create-namespace \
-f - <<EOF
global:
  hub: us-docker.pkg.dev/istio-enterprise-private/internal-istio-builds
  proxy:
    clusterDomain: cluster.local
  tag: 1.24-alpha.a2295ca05a358e7c8e9edbbd3f500c8b4eb11864
  multiCluster:
    clusterName: cluster2
profile: ambient
istio_cni:
  enabled: true
meshConfig:
  accessLogFile: /dev/stdout
  defaultConfig:
    proxyMetadata:
      ISTIO_META_DNS_AUTO_ALLOCATE: "true"
      ISTIO_META_DNS_CAPTURE: "true"
  trustDomain: cluster.local
pilot:
  enabled: true
  env:
    PILOT_ENABLE_IP_AUTOALLOCATE: "true"
    PILOT_ENABLE_K8S_SELECT_WORKLOAD_ENTRIES: "false"
    PILOT_SKIP_VALIDATE_TRUST_DOMAIN: "true"
  podLabels:
    hack: eastwest
  platforms:
    peering:
      enabled: true
EOF

helm upgrade --install istio-cni oci://us-docker.pkg.dev/istio-enterprise-private/internal-istio-helm/cni \
--namespace kube-system \
--kube-context=${CLUSTER2} \
--version 1.24-alpha.a2295ca05a358e7c8e9edbbd3f500c8b4eb11864 \
--create-namespace \
-f - <<EOF
global:
  hub: us-docker.pkg.dev/istio-enterprise-private/internal-istio-builds
  proxy: 1.24-alpha.a2295ca05a358e7c8e9edbbd3f500c8b4eb11864
profile: ambient
cni:
  ambient:
    dnsCapture: true
  excludeNamespaces:
  - istio-system
  - kube-system
EOF

helm upgrade --install ztunnel oci://us-docker.pkg.dev/istio-enterprise-private/internal-istio-helm/ztunnel \
--namespace istio-system \
--kube-context=${CLUSTER2} \
--version 1.24-alpha.a2295ca05a358e7c8e9edbbd3f500c8b4eb11864 \
--create-namespace \
-f - <<EOF
configValidation: true
enabled: true
env:
  L7_ENABLED: "true"
  NETWORK: cluster2
hub: us-docker.pkg.dev/istio-enterprise-private/internal-istio-builds
istioNamespace: istio-system
multiCluster:
  clusterName: cluster2
namespace: istio-system
profile: ambient
proxy:
  clusterDomain: cluster.local
tag: 1.24-alpha.a2295ca05a358e7c8e9edbbd3f500c8b4eb11864
terminationGracePeriodSeconds: 29
variant: distroless
EOF

helm upgrade --install istio-ingressgateway- oci://us-docker.pkg.dev/istio-enterprise-private/internal-istio-helm/gateway \
--namespace istio-gateways \
--kube-context=${CLUSTER2} \
--version 1.24-alpha.a2295ca05a358e7c8e9edbbd3f500c8b4eb11864 \
--create-namespace \
-f - <<EOF
autoscaling:
  enabled: false
profile: ambient
imagePullPolicy: IfNotPresent
labels:
  app: istio-ingressgateway
  istio: ingressgateway
service:
  type: None
EOF

helm upgrade --install istio-eastwestgateway- oci://us-docker.pkg.dev/istio-enterprise-private/internal-istio-helm/gateway \
--namespace istio-gateways \
--kube-context=${CLUSTER2} \
--version 1.24-alpha.a2295ca05a358e7c8e9edbbd3f500c8b4eb11864 \
--create-namespace \
-f - <<EOF
autoscaling:
  enabled: false
profile: ambient
imagePullPolicy: IfNotPresent
env:
  ISTIO_META_REQUESTED_NETWORK_VIEW: cluster2
  ISTIO_META_ROUTER_MODE: sni-dnat
labels:
  app: istio-ingressgateway
  istio: eastwestgateway
  topology.istio.io/network: cluster2
service:
  type: None
EOF
```
The Gateway APIs do not come installed by default on most Kubernetes clusters. Install the Gateway API CRDs if they are not present:
```bash
kubectl --context ${CLUSTER1} get crd gateways.gateway.networking.k8s.io &> /dev/null || \
  { kubectl kustomize "github.com/kubernetes-sigs/gateway-api/config/crd?ref=v1.1.0" | kubectl --context ${CLUSTER1} apply -f -; }
kubectl --context ${CLUSTER2} get crd gateways.gateway.networking.k8s.io &> /dev/null || \
  { kubectl kustomize "github.com/kubernetes-sigs/gateway-api/config/crd?ref=v1.1.0" | kubectl --context ${CLUSTER2} apply -f -; }
```

<!--bash
cat <<'EOF' > ./test.js

const helpers = require('./tests/chai-exec');

const chaiExec = require("@jsdevtools/chai-exec");
const helpersHttp = require('./tests/chai-http');
const chai = require("chai");
const expect = chai.expect;

afterEach(function (done) {
  if (this.currentTest.currentRetry() > 0) {
    process.stdout.write(".");
    setTimeout(done, 1000);
  } else {
    done();
  }
});

describe("Checking Istio installation", function() {
  it('istiod pods are ready in cluster ' + process.env.CLUSTER1, () => helpers.checkDeploymentsWithLabels({ context: process.env.CLUSTER1, namespace: "istio-system", labels: "app=istiod", instances: 1 }));
  it('gateway pods are ready in cluster ' + process.env.CLUSTER1, () => helpers.checkDeploymentsWithLabels({ context: process.env.CLUSTER1, namespace: "istio-gateways", labels: "app=istio-ingressgateway", instances: 2 }));
  it('istiod pods are ready in cluster ' + process.env.CLUSTER2, () => helpers.checkDeploymentsWithLabels({ context: process.env.CLUSTER2, namespace: "istio-system", labels: "app=istiod", instances: 1 }));
  it('gateway pods are ready in cluster ' + process.env.CLUSTER2, () => helpers.checkDeploymentsWithLabels({ context: process.env.CLUSTER2, namespace: "istio-gateways", labels: "app=istio-ingressgateway", instances: 2 }));
  it("Gateways have an ip attached in cluster " + process.env.CLUSTER1, () => {
    let cli = chaiExec("kubectl --context " + process.env.CLUSTER1 + " -n istio-gateways get svc -l app=istio-ingressgateway -o jsonpath='{.items}'");
    cli.stderr.should.be.empty;
    let deployments = JSON.parse(cli.stdout.slice(1,-1));
    expect(deployments).to.have.lengthOf(2);
    deployments.forEach((deployment) => {
      expect(deployment.status.loadBalancer).to.have.property("ingress");
    });
  });
  it("Gateways have an ip attached in cluster " + process.env.CLUSTER2, () => {
    let cli = chaiExec("kubectl --context " + process.env.CLUSTER2 + " -n istio-gateways get svc -l app=istio-ingressgateway -o jsonpath='{.items}'");
    cli.stderr.should.be.empty;
    let deployments = JSON.parse(cli.stdout.slice(1,-1));
    expect(deployments).to.have.lengthOf(2);
    deployments.forEach((deployment) => {
      expect(deployment.status.loadBalancer).to.have.property("ingress");
    });
  });
});

EOF
echo "executing test dist/gloo-mesh-2-0-workshop/build/templates/steps/deploy-istio-helm/tests/istio-ready.test.js.liquid"
timeout --signal=INT 3m mocha ./test.js --timeout 10000 --retries=120 --bail || { DEBUG_MODE=true mocha ./test.js --timeout 120000; exit 1; }
-->
<!--bash
timeout 2m bash -c "until [[ \$(kubectl --context ${CLUSTER1} -n istio-gateways get svc -l istio=ingressgateway -o json | jq '.items[0].status.loadBalancer | length') -gt 0 ]]; do
  sleep 1
done"
-->

```bash
export HOST_GW_CLUSTER1="$(kubectl --context ${CLUSTER1} -n istio-gateways get svc -l istio=ingressgateway -o jsonpath='{.items[0].status.loadBalancer.ingress[0].*}')"
export HOST_GW_CLUSTER2="$(kubectl --context ${CLUSTER2} -n istio-gateways get svc -l istio=ingressgateway -o jsonpath='{.items[0].status.loadBalancer.ingress[0].*}')"
```

<!--bash
cat <<'EOF' > ./test.js
const dns = require('dns');
const chaiHttp = require("chai-http");
const chai = require("chai");
const expect = chai.expect;
chai.use(chaiHttp);
const { waitOnFailedTest } = require('./tests/utils');

afterEach(function(done) { waitOnFailedTest(done, this.currentTest.currentRetry())});

describe("Address '" + process.env.HOST_GW_CLUSTER1 + "' can be resolved in DNS", () => {
    it(process.env.HOST_GW_CLUSTER1 + ' can be resolved', (done) => {
        return dns.lookup(process.env.HOST_GW_CLUSTER1, (err, address, family) => {
            expect(address).to.be.an.ip;
            done();
        });
    });
});
EOF
echo "executing test ./default/tests/can-resolve.test.js.liquid"
timeout --signal=INT 3m mocha ./test.js --timeout 10000 --retries=120 --bail || { DEBUG_MODE=true mocha ./test.js --timeout 120000; exit 1; }
-->
<!--bash
cat <<'EOF' > ./test.js
const dns = require('dns');
const chaiHttp = require("chai-http");
const chai = require("chai");
const expect = chai.expect;
chai.use(chaiHttp);
const { waitOnFailedTest } = require('./tests/utils');

afterEach(function(done) { waitOnFailedTest(done, this.currentTest.currentRetry())});

describe("Address '" + process.env.HOST_GW_CLUSTER2 + "' can be resolved in DNS", () => {
    it(process.env.HOST_GW_CLUSTER2 + ' can be resolved', (done) => {
        return dns.lookup(process.env.HOST_GW_CLUSTER2, (err, address, family) => {
            expect(address).to.be.an.ip;
            done();
        });
    });
});
EOF
echo "executing test ./default/tests/can-resolve.test.js.liquid"
timeout --signal=INT 3m mocha ./test.js --timeout 10000 --retries=120 --bail || { DEBUG_MODE=true mocha ./test.js --timeout 120000; exit 1; }
-->






## Lab 5 - Deploy the httpbin demo app <a name="lab-5---deploy-the-httpbin-demo-app-"></a>
[<img src="https://img.youtube.com/vi/w1xB-o_gHs0/maxresdefault.jpg" alt="VIDEO LINK" width="560" height="315"/>](https://youtu.be/w1xB-o_gHs0 "Video Link")


We're going to deploy the httpbin application to demonstrate several features of Gloo Mesh on cluster CLUSTER1.

You can find more information about this application [here](http://httpbin.org/).

Run the following commands to deploy the httpbin app on `cluster1`. The deployment will be called `not-in-mesh` and won't have the sidecar injected, because of the annotation `sidecar.istio.io/inject: "false"` and its traffic won't be redirected to ztunnel because of the annotation `istio.io/dataplane-mode: none`.

```bash
kubectl --context ${CLUSTER1} create ns httpbin
kubectl --context ${CLUSTER1} label namespace httpbin istio.io/dataplane-mode=ambient
kubectl apply --context ${CLUSTER1} -f - <<EOF

apiVersion: v1
kind: Service
metadata:
  name: not-in-mesh
  namespace: httpbin
  labels:
    app: not-in-mesh
    service: not-in-mesh
spec:
  ports:
  - name: http
    port: 8000
    targetPort: 80
  selector:
    app: not-in-mesh
---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: not-in-mesh
  namespace: httpbin
spec:
  replicas: 1
  selector:
    matchLabels:
      app: not-in-mesh
      version: v1
  template:
    metadata:
      labels:
        app: not-in-mesh
        version: v1
        istio.io/dataplane-mode: none
        sidecar.istio.io/inject: "false"
    spec:
      containers:
      - image: docker.io/kennethreitz/httpbin
        imagePullPolicy: IfNotPresent
        name: not-in-mesh
        ports:
        - name: http
          containerPort: 80
        livenessProbe:
          httpGet:
            path: /status/200
            port: http
        readinessProbe:
          httpGet:
            path: /status/200
            port: http

EOF
```

Then, we deploy a second version, which will be called `in-mesh` and will be part of the mesh.

```bash
kubectl apply --context ${CLUSTER1} -f - <<EOF

apiVersion: v1
kind: Service
metadata:
  name: in-mesh
  namespace: httpbin
  labels:
    app: in-mesh
    service: in-mesh
spec:
  ports:
  - name: http
    port: 8000
    targetPort: 80
  selector:
    app: in-mesh
---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: in-mesh
  namespace: httpbin
spec:
  replicas: 1
  selector:
    matchLabels:
      app: in-mesh
      version: v1
  template:
    metadata:
      labels:
        app: in-mesh
        version: v1
        sidecar.istio.io/inject: "true"
    spec:
      containers:
      - image: docker.io/kennethreitz/httpbin
        imagePullPolicy: IfNotPresent
        name: in-mesh
        ports:
        - name: http
          containerPort: 80
        livenessProbe:
          httpGet:
            path: /status/200
            port: http
        readinessProbe:
          httpGet:
            path: /status/200
            port: http

EOF
```


<!--bash
echo -n Waiting for httpbin pods to be ready...
timeout -v 5m bash -c "
until [[ \$(kubectl --context ${CLUSTER1} -n httpbin get deploy -o json | jq '[.items[].status.readyReplicas] | add') -eq 2 ]] 2>/dev/null
do
  sleep 1
  echo -n .
done"
echo
-->
Add another HTTPBin service which is deployed in Ambient.
```bash
kubectl apply --context ${CLUSTER1} -f - <<EOF

apiVersion: v1
kind: Service
metadata:
  name: in-ambient
  namespace: httpbin
  labels:
    app: in-ambient
    service: in-ambient
spec:
  ports:
  - name: http
    port: 8000
    targetPort: 80
  selector:
    app: in-ambient
---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: in-ambient
  namespace: httpbin
spec:
  replicas: 1
  selector:
    matchLabels:
      app: in-ambient
      version: v1
  template:
    metadata:
      labels:
        app: in-ambient
        version: v1
        istio.io/dataplane-mode: ambient
        sidecar.istio.io/inject: "false"
        istio-injection: disabled
    spec:
      containers:
      - image: docker.io/kennethreitz/httpbin
        imagePullPolicy: IfNotPresent
        name: in-ambient
        ports:
        - name: http
          containerPort: 80
        livenessProbe:
          httpGet:
            path: /status/200
            port: http
        readinessProbe:
          httpGet:
            path: /status/200
            port: http

EOF
```
You can follow the progress using the following command:

```bash
kubectl --context ${CLUSTER1} -n httpbin get pods
```

```,nocopy
NAME                           READY   STATUS    RESTARTS   AGE
in-mesh-5d9d9549b5-qrdgd       2/2     Running   0          11s
in-ambient-5c64bb49cd-m9kwm    1/1     Running   0          4s
not-in-mesh-5c64bb49cd-m9kwm   1/1     Running   0          11s
```
<!--bash
cat <<'EOF' > ./test.js
const helpers = require('./tests/chai-exec');

describe("httpbin app", () => {
  let cluster = process.env.CLUSTER1
  
  let deployments = ["not-in-mesh", "in-mesh"];
  
  deployments.forEach(deploy => {
    it(deploy + ' pods are ready in ' + cluster, () => helpers.checkDeployment({ context: cluster, namespace: "httpbin", k8sObj: deploy }));
  });
});
EOF
echo "executing test dist/gloo-mesh-2-0-workshop/build/templates/steps/apps/httpbin/deploy-httpbin/tests/check-httpbin.test.js.liquid"
timeout --signal=INT 3m mocha ./test.js --timeout 10000 --retries=120 --bail || { DEBUG_MODE=true mocha ./test.js --timeout 120000; exit 1; }
-->



## Lab 6 - Deploy the httpbin demo app <a name="lab-6---deploy-the-httpbin-demo-app-"></a>
[<img src="https://img.youtube.com/vi/w1xB-o_gHs0/maxresdefault.jpg" alt="VIDEO LINK" width="560" height="315"/>](https://youtu.be/w1xB-o_gHs0 "Video Link")


We're going to deploy the httpbin application to demonstrate several features of Gloo Mesh on cluster CLUSTER2.

You can find more information about this application [here](http://httpbin.org/).

Run the following commands to deploy the httpbin app on `cluster1`. The deployment will be called `not-in-mesh` and won't have the sidecar injected, because of the annotation `sidecar.istio.io/inject: "false"` and its traffic won't be redirected to ztunnel because of the annotation `istio.io/dataplane-mode: none`.

```bash
kubectl --context ${CLUSTER2} create ns httpbin
kubectl --context ${CLUSTER2} label namespace httpbin istio.io/dataplane-mode=ambient
kubectl apply --context ${CLUSTER2} -f - <<EOF

apiVersion: v1
kind: Service
metadata:
  name: not-in-mesh
  namespace: httpbin
  labels:
    app: not-in-mesh
    service: not-in-mesh
spec:
  ports:
  - name: http
    port: 8000
    targetPort: 80
  selector:
    app: not-in-mesh
---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: not-in-mesh
  namespace: httpbin
spec:
  replicas: 1
  selector:
    matchLabels:
      app: not-in-mesh
      version: v1
  template:
    metadata:
      labels:
        app: not-in-mesh
        version: v1
        istio.io/dataplane-mode: none
        sidecar.istio.io/inject: "false"
    spec:
      containers:
      - image: docker.io/kennethreitz/httpbin
        imagePullPolicy: IfNotPresent
        name: not-in-mesh
        ports:
        - name: http
          containerPort: 80
        livenessProbe:
          httpGet:
            path: /status/200
            port: http
        readinessProbe:
          httpGet:
            path: /status/200
            port: http

EOF
```

Then, we deploy a second version, which will be called `in-mesh` and will be part of the mesh.

```bash
kubectl apply --context ${CLUSTER2} -f - <<EOF

apiVersion: v1
kind: Service
metadata:
  name: in-mesh
  namespace: httpbin
  labels:
    app: in-mesh
    service: in-mesh
spec:
  ports:
  - name: http
    port: 8000
    targetPort: 80
  selector:
    app: in-mesh
---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: in-mesh
  namespace: httpbin
spec:
  replicas: 1
  selector:
    matchLabels:
      app: in-mesh
      version: v1
  template:
    metadata:
      labels:
        app: in-mesh
        version: v1
        sidecar.istio.io/inject: "true"
    spec:
      containers:
      - image: docker.io/kennethreitz/httpbin
        imagePullPolicy: IfNotPresent
        name: in-mesh
        ports:
        - name: http
          containerPort: 80
        livenessProbe:
          httpGet:
            path: /status/200
            port: http
        readinessProbe:
          httpGet:
            path: /status/200
            port: http

EOF
```


<!--bash
echo -n Waiting for httpbin pods to be ready...
timeout -v 5m bash -c "
until [[ \$(kubectl --context ${CLUSTER2} -n httpbin get deploy -o json | jq '[.items[].status.readyReplicas] | add') -eq 2 ]] 2>/dev/null
do
  sleep 1
  echo -n .
done"
echo
-->
Add another HTTPBin service which is deployed in Ambient.
```bash
kubectl apply --context ${CLUSTER2} -f - <<EOF

apiVersion: v1
kind: Service
metadata:
  name: in-ambient
  namespace: httpbin
  labels:
    app: in-ambient
    service: in-ambient
spec:
  ports:
  - name: http
    port: 8000
    targetPort: 80
  selector:
    app: in-ambient
---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: in-ambient
  namespace: httpbin
spec:
  replicas: 1
  selector:
    matchLabels:
      app: in-ambient
      version: v1
  template:
    metadata:
      labels:
        app: in-ambient
        version: v1
        istio.io/dataplane-mode: ambient
        sidecar.istio.io/inject: "false"
        istio-injection: disabled
    spec:
      containers:
      - image: docker.io/kennethreitz/httpbin
        imagePullPolicy: IfNotPresent
        name: in-ambient
        ports:
        - name: http
          containerPort: 80
        livenessProbe:
          httpGet:
            path: /status/200
            port: http
        readinessProbe:
          httpGet:
            path: /status/200
            port: http

EOF
```
You can follow the progress using the following command:

```bash
kubectl --context ${CLUSTER2} -n httpbin get pods
```

```,nocopy
NAME                           READY   STATUS    RESTARTS   AGE
in-mesh-5d9d9549b5-qrdgd       2/2     Running   0          11s
in-ambient-5c64bb49cd-m9kwm    1/1     Running   0          4s
not-in-mesh-5c64bb49cd-m9kwm   1/1     Running   0          11s
```
<!--bash
cat <<'EOF' > ./test.js
const helpers = require('./tests/chai-exec');

describe("httpbin app", () => {
  let cluster = process.env.CLUSTER1
  
  let deployments = ["not-in-mesh", "in-mesh"];
  
  deployments.forEach(deploy => {
    it(deploy + ' pods are ready in ' + cluster, () => helpers.checkDeployment({ context: cluster, namespace: "httpbin", k8sObj: deploy }));
  });
});
EOF
echo "executing test dist/gloo-mesh-2-0-workshop/build/templates/steps/apps/httpbin/deploy-httpbin/tests/check-httpbin.test.js.liquid"
timeout --signal=INT 3m mocha ./test.js --timeout 10000 --retries=120 --bail || { DEBUG_MODE=true mocha ./test.js --timeout 120000; exit 1; }
-->



## Lab 7 - Deploy the clients to make requests to other services <a name="lab-7---deploy-the-clients-to-make-requests-to-other-services-"></a>

We're going to deploy services that we'll use as clients to demonstrate several features of Gloo Mesh.

Run the following commands to deploy the client on `cluster1`. The deployment will be called `not-in-mesh` and won't have the sidecar injected, because of the annotation `sidecar.istio.io/inject: "false"` and its traffic won't be redirected to ztunnel because of the annotation `istio.io/dataplane-mode: none`.

```bash
kubectl --context ${CLUSTER1} create ns clients

kubectl apply --context ${CLUSTER1} -f - <<EOF
apiVersion: v1
kind: ServiceAccount
metadata:
  name: not-in-mesh
  namespace: clients
---
apiVersion: v1
kind: Service
metadata:
  name: not-in-mesh
  namespace: clients
  labels:
    app: not-in-mesh
    service: not-in-mesh
spec:
  ports:
  - name: http
    port: 8000
    targetPort: 80
  selector:
    app: not-in-mesh
---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: not-in-mesh
  namespace: clients
spec:
  replicas: 1
  selector:
    matchLabels:
      app: not-in-mesh
      version: v1
  template:
    metadata:
      labels:
        app: not-in-mesh
        version: v1
        istio.io/dataplane-mode: none
        sidecar.istio.io/inject: "false"
    spec:
      serviceAccountName: not-in-mesh
      containers:
      - image: nicolaka/netshoot:latest
        imagePullPolicy: IfNotPresent
        name: netshoot
        command: ["/bin/bash"]
        args: ["-c", "while true; do ping localhost; sleep 60;done"]
EOF
```
Then, we deploy a second version, which will be called `in-mesh-with-sidecar` and will have the sidecar injected (because of the label `istio-injection` in the Pod template)

```bash
kubectl apply --context ${CLUSTER1} -f - <<EOF
apiVersion: v1
kind: ServiceAccount
metadata:
  name: in-mesh-with-sidecar
  namespace: clients
---
apiVersion: v1
kind: Service
metadata:
  name: in-mesh-with-sidecar
  namespace: clients
  labels:
    app: in-mesh-with-sidecar
    service: in-mesh-with-sidecar
spec:
  ports:
  - name: http
    port: 8000
    targetPort: 80
  selector:
    app: in-mesh-with-sidecar
---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: in-mesh-with-sidecar
  namespace: clients
spec:
  replicas: 1
  selector:
    matchLabels:
      app: in-mesh-with-sidecar
      version: v1
  template:
    metadata:
      labels:
        app: in-mesh-with-sidecar
        version: v1
        sidecar.istio.io/inject: "true"
    spec:
      serviceAccountName: in-mesh-with-sidecar
      containers:
      - image: nicolaka/netshoot:latest
        imagePullPolicy: IfNotPresent
        name: netshoot
        command: ["/bin/bash"]
        args: ["-c", "while true; do ping localhost; sleep 60;done"]
EOF
```

<!--bash
echo -n Waiting for clients to be ready...
timeout -v 5m bash -c "
until [[ \$(kubectl --context ${CLUSTER1} -n clients get deploy -o json | jq '[.items[].status.readyReplicas] | add') -eq 2 ]] 2>/dev/null
do
  sleep 1
  echo -n .
done"
echo
-->
Add another client service which is deployed in Ambient.

```bash
kubectl apply --context ${CLUSTER1} -f - <<EOF
apiVersion: v1
kind: ServiceAccount
metadata:
  name: in-ambient
  namespace: clients
---
apiVersion: v1
kind: Service
metadata:
  name: in-ambient
  namespace: clients
  labels:
    app: in-ambient
    service: in-ambient
spec:
  ports:
  - name: http
    port: 8000
    targetPort: 80
  selector:
    app: in-ambient
---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: in-ambient
  namespace: clients
spec:
  replicas: 1
  selector:
    matchLabels:
      app: in-ambient
      version: v1
  template:
    metadata:
      labels:
        app: in-ambient
        version: v1
        istio.io/dataplane-mode: ambient
        sidecar.istio.io/inject: "false"
        istio-injection: disabled
    spec:
      serviceAccountName: in-ambient
      containers:
      - image: nicolaka/netshoot:latest
        imagePullPolicy: IfNotPresent
        name: netshoot
        command: ["/bin/bash"]
        args: ["-c", "while true; do ping localhost; sleep 60;done"]
EOF
```
You can follow the progress using the following command:

```bash
kubectl --context ${CLUSTER1} -n clients get pods
```

```,nocopy
NAME                           READY   STATUS    RESTARTS   AGE
in-ambient-5c64bb49cd-w3dmw    1/1     Running   0          4s
in-mesh-5d9d9549b5-qrdgd       2/2     Running   0          11s
not-in-mesh-5c64bb49cd-m9kwm   1/1     Running   0          11s
```
<!--bash
cat <<'EOF' > ./test.js
const helpers = require('./tests/chai-exec');

describe("client apps", () => {
  let cluster = process.env.CLUSTER1
  
  let deployments = ["not-in-mesh", "in-mesh-with-sidecar", "in-ambient"];
  
  deployments.forEach(deploy => {
    it(deploy + ' pods are ready in ' + cluster, () => helpers.checkDeployment({ context: cluster, namespace: "clients", k8sObj: deploy }));
  });
});
EOF
echo "executing test dist/gloo-mesh-2-0-workshop/build/templates/steps/apps/clients/deploy-clients/tests/check-clients.test.js.liquid"
timeout --signal=INT 3m mocha ./test.js --timeout 10000 --retries=120 --bail || { DEBUG_MODE=true mocha ./test.js --timeout 120000; exit 1; }
-->



## Lab 8 - Link Clusters <a name="lab-8---link-clusters-"></a>

Install the Kubernetes Gateway API in both clusters:
```
kubectl --context $CLUSTER1 apply -f https://github.com/kubernetes-sigs/gateway-api/releases/download/v1.2.0/standard-install.yaml
kubectl --context $CLUSTER2 apply -f https://github.com/kubernetes-sigs/gateway-api/releases/download/v1.2.0/standard-install.yaml
```

Configure the network topology for each cluster in the `istio-system` namespace:

```bash
kubectl --context $CLUSTER1 label namespace istio-system topology.istio.io/network=$CLUSTER1
kubectl --context $CLUSTER2 label namespace istio-system topology.istio.io/network=$CLUSTER2
```

Create a Gateway for each cluster:

```bash
  cat <<EOF | kubectl --context $CLUSTER1 apply -f -
apiVersion: gateway.networking.k8s.io/v1
kind: Gateway
metadata:
  name: eastwest
  namespace: istio-system
  labels:
    topology.istio.io/network: $CLUSTER1
    istio.io/expose-istiod: "15012"
spec:
  gatewayClassName: istio-eastwest
  listeners:
  - name: cross-network
    port: 15008
    protocol: HBONE
    tls:
      mode: Passthrough
EOF

  cat <<EOF | kubectl --context $CLUSTER2 apply -f -
apiVersion: gateway.networking.k8s.io/v1
kind: Gateway
metadata:
  name: eastwest
  namespace: istio-system
  labels:
    topology.istio.io/network: $CLUSTER2
    istio.io/expose-istiod: "15012"
spec:
  gatewayClassName: istio-eastwest
  listeners:
  - name: cross-network
    port: 15008
    protocol: HBONE
    tls:
      mode: Passthrough
EOF
```

Patch the eastwest gateways to admit xds traffic to the clusters:

```bash
kubectl --context $CLUSTER1 -n istio-system patch svc eastwest --type='json' -p='[{"op": "add", "path": "/spec/ports/-", "value": {"name": "grpc-xds", "port": 15010, "targetPort": "grpc-xds", "protocol": "TCP"}}]'
kubectl --context $CLUSTER2 -n istio-system patch svc eastwest --type='json' -p='[{"op": "add", "path": "/spec/ports/-", "value": {"name": "grpc-xds", "port": 15010, "targetPort": "grpc-xds", "protocol": "TCP"}}]'
```

Link the first cluster to the second cluster:

```bash
CLUSTER_GW_IP="$(kubectl --context $CLUSTER2 -n istio-system get service eastwest -o jsonpath='{.status.loadBalancer.ingress[0].*}')"

cat << EOF | kubectl --context ${CLUSTER1} apply -f -
apiVersion: gateway.networking.k8s.io/v1
kind: Gateway
metadata:
  annotations:
    gateway.istio.io/service-account: eastwest
  labels:
    topology.istio.io/network: cluster2
  name: remote-cluster2
  namespace: istio-system
spec:
  addresses:
  - type: IPAddress
    value: "${CLUSTER_GW_IP}"
  gatewayClassName: istio-remote
  listeners:
  - name: cross-network
    port: 15008
    protocol: HBONE
EOF
```

Link the second cluster to the first cluster:

```bash
CLUSTER_GW_IP="$(kubectl --context $CLUSTER1 -n istio-system get service eastwest -o jsonpath='{.status.loadBalancer.ingress[0].*}')"

cat << EOF | kubectl --context ${CLUSTER1} apply -f -
apiVersion: gateway.networking.k8s.io/v1
kind: Gateway
metadata:
  annotations:
    gateway.istio.io/service-account: eastwest
  labels:
    topology.istio.io/network: cluster2
  name: remote-cluster2
  namespace: istio-system
spec:
  addresses:
  - type: IPAddress
    value: "${CLUSTER_GW_IP}"
  gatewayClassName: istio-remote
  listeners:
  - name: cross-network
    port: 15008
    protocol: HBONE
EOF
```

Let's validate that cross cluster traffic works. Let's annotate services taht are globally accessible:

Next expose global service.

```bash
kubectl --context $CLUSTER1 -n httpbin annotate svc in-ambient istio.io/global-service=true
kubectl --context $CLUSTER2 -n httpbin annotate svc in-ambient istio.io/global-service=true
```

Validate that Service Entries are workload entries are created to route traffic to the gateway of the other cluster:

```shell
kubectl --context $CLUSTER1 -n httpbin get serviceentry
kubectl --context $CLUSTER2 -n httpbin get workloadentry
```

> [!IMPORTANT]
> Limitations:
>
> * Workloads have to use the default service account.
> * Multi-Network traffic is currently not supported by Istio Gateways, Sidecars, and Waypoints.

Next, let's send some traffic across the clusters:

```shell
kubectl --context=$CLUSTER1 -n clients exec -it deploy/in-ambient -- curl -v in-ambient.httpbin.global:8000/get
```

<!--bash
cat <<'EOF' > ./test.js
const chaiExec = require("@jsdevtools/chai-exec");
var chai = require('chai');
var expect = chai.expect;
chai.use(chaiExec);
const helpers = require('./tests/chai-exec');


describe("ensure traffic goes to workloads in both clusters", () => {
  it('should have two origins', async () => {
    const origins = new Set();
    for (let i = 0; i < 10; i++) {
      const command = await helpers.curlInDeployment({
        curlCommand: 'curl in-ambient.httpbin.global:8000/get',
        deploymentName: 'in-ambient',
        namespace: 'clients',
        context: `${process.env.CLUSTER1}`
      });
      const origin = JSON.parse(command).origin;
      origins.add(origin);
    }
    expect(origins.size).to.equal(2);
  });
});

EOF
echo "executing test dist/gloo-mesh-2-0-workshop/build/templates/steps/link-clusters/tests/check-cross-cluster-traffic.js.liquid"
timeout --signal=INT 3m mocha ./test.js --timeout 10000 --retries=120 --bail || { DEBUG_MODE=true mocha ./test.js --timeout 120000; exit 1; }
-->



